ABSTRACT
Neural Networks today find their application in variety of fields, from Computer Vision to Natural Language
processing, from Speech Recognition to classification. But they are computationally expensive, memory intensive
and difficult to deploy on embedded systems. To attempt to solve this problem, the project will present the survey
of degree of compression and the performance of various Deep Neural Network(DNN) compression techniques
like Quantization, Pruning and Knowledge Distillation(KD). The motivation for the proposed project is to leverage
some/all of these DNN acceleration approaches to reduce the size of DNN model with little to no loss of accuracy
to make the baseline models as fast and compressed as possible to enable the use of energy-efficient and real-time
processing DNNs on embedded systems. The compressed models will be benchmarked on CPU and GPU using
MNIST and CIFAR-10 datasets.
